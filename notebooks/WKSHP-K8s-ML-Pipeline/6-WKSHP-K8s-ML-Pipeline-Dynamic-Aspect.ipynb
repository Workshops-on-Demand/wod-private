{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline with KubeDirector - Lab 6\n",
    "## Dynamic? Did someone say a dynamic ML pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lab workflow**\n",
    "\n",
    "Over time, your model might get stale because of changing traffic conditions and a changing dataset. You may also want to enhance the performance of your model as a result of monitoring the accuracy of the predictions over time. \n",
    "\n",
    "To illustrate the dynamic aspect of the ML pipeline you have just built, letâ€™s imagine you want to improve the prediction accuracy of your model. \n",
    "\n",
    "In this lab: \n",
    "\n",
    "1. You will **retrain** your model by tuning the model parameters to get better predictive performance and save your retrained model into a new file. \n",
    "\n",
    "2. You will then update the model registry information in the configMap kubernetes resource with the file path for the newly trained model file and associated scoring script file. \n",
    "\n",
    "3. You will finally make a new prediction query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1- Initialize the environment**\n",
    "\n",
    "Let's first define the environment variables needed to execute this part of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# environment variables\n",
    "#\n",
    "studentId=\"student{{ STDID }}\" # your Jupyter Notebook student Identifier (i.e.: student<xx>)\n",
    "\n",
    "gateway_host=\"{{ HPEECPGWNAME }}\"\n",
    "Internet_access=\"{{ JPHOSTEXT }}\"\n",
    "\n",
    "kc_secret=\"kc-secret-students.yaml\" #the kubeconfig secret object.\n",
    "JupyterNotebookApp=\"cr-cluster-jupyter-notebook.yaml\" # the Jupyter Notebook KD App manifest you will deploy to build your model\n",
    "DeploymentEngineApp=\"cr-cluster-endpoint-wrapper.yaml\" # the Deployment engine KD App manifest you will deploy to query your model for answers \n",
    "PipelineConfigMap=\"ml-pipeline-configmap.yaml\" # ConfigMap manifest used to register the trained model version 1 \n",
    "PipelineConfigMapv2=\"ml-pipeline-configmap-v2.yaml\" # ConfigMap manifest used to register the trained model version 2 \n",
    "#\n",
    "clusterName=\"inference-server-${studentId}\"\n",
    "#\n",
    "# Model registry information\n",
    "#\n",
    "TrainingModel=\"model-${studentId}\"\n",
    "modelVersion=\"1\"\n",
    "#\n",
    "echo \"Your studentId is: \"$studentId "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2- Retrain your model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">Go back to your **local Jupyter Notebook**, Lab Part 3, and run the code cell from the step **\"6-Retrain the model to improve model accuracy\"**.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3- Adjust the model registry information**\n",
    "\n",
    "Once your model is retrained, adjust the model registry information.\n",
    "Run the code cells below to update the model registry information with the path of your **retrained** model file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat $PipelineConfigMapv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f $PipelineConfigMapv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then, use the command `kubectl describe configmap` below and check the events logged against it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe configmap $TrainingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the events section at the bottom of the command output, you should notice the message:  \n",
    "_\"Connected to cluster {your deployment-engine cluster name}; updating it.\"_ \n",
    "\n",
    ">**Note:** KubeDirector has detected the change of the model registry information. KubeDirector then immediately updates the model registry metadata information on the PODs/Containers of your deployment engine cluster. The deployment engine cluster can therefore reference the newly trained model file to use to serve the prediction requests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And use the command `kubectl describe kdcluster` below and check the events logged against it:** \n",
    "\n",
    "In the events section at the bottom of the command output, you should notice the message:  \n",
    "\n",
    "_\"connected configmap has changes, updated context for the PODs of your instance of the deployment engine cluster\"._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl describe kdcluster $clusterName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** By changing the configMap, your entire ML pipeline will be reconciled by KubeDirector operator for you, while the containers of the _deployment engine_ cluster environment remain running. \n",
    "In terms of the use case here, that means you can update existing model registry information or add another model to be handled by the deployment engine without interrupting any current requests that the deployment engine environment is processing.   \n",
    "**The key use of the KubeDirector applications, the KubeDirector clusters, and the KubeDirector Connections capability is what makes your ML pipeline very _dynamic_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make another query using your retrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Getting the Model Serving access point from the haproxy service of the LoadBalancer (role: LoadBalancer, internal port: 32700):\n",
    "#\n",
    "LoadBalancerURL=$(kubectl describe service -l kubedirector.hpe.com/kdcluster=${clusterName},kubedirector.hpe.com/role=LoadBalancer | grep gateway/32700 | awk '{print $2}')\n",
    "LoadBalancerPort=$(echo $LoadBalancerURL | cut -d':' -f 2) # extract the gateway re-mapped port value.\n",
    "LoadBalancer_endpoint=\"https://$gateway_host:$LoadBalancerPort\"\n",
    "LoadBalancerAuthToken=$(kubectl describe service -l kubedirector.hpe.com/kdcluster=${clusterName},kubedirector.hpe.com/role=LoadBalancer | grep kd-auth-token  | awk '{print $2}' | tr -d '\\r')\n",
    "#\n",
    "# REST API query:\n",
    "#\n",
    "curl --location -k -s --request POST \"${LoadBalancer_endpoint}/${TrainingModel}/${modelVersion}/predict\" \\\n",
    "--header \"X-Auth-Token: ${LoadBalancerAuthToken}\" \\\n",
    "--header 'Content-Type: application/json' \\\n",
    "--data-raw '{\n",
    "    \"use_scoring\": true,\n",
    "    \"scoring_args\": {\n",
    "        \"work\": 0,\n",
    "        \"start_latitude\": 40.57689727,\n",
    "        \"start_longitude\": -73.99047356,\n",
    "        \"end_latitude\": 40.72058154,\n",
    "        \"end_longitude\": -73.99740673,\n",
    "        \"distance\": 8,\n",
    "        \"weekday\": 1,\n",
    "        \"hour\": 9,\n",
    "        \"month_1\": 0,\n",
    "        \"month_2\": 1,\n",
    "        \"month_3\": 0,\n",
    "        \"month_4\": 0,\n",
    "        \"month_5\": 0,\n",
    "        \"month_6\": 0\n",
    "    }\n",
    "}' | python -m json.tool | grep output | cut -d'\\' -f 3 | cut -c 2-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4- Time to go through some cleanup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete your deployment engine, configMap, and your local Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete -f $kc_secret\n",
    "rm -f $kc_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete -f $DeploymentEngineApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete -f $PipelineConfigMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete -f $JupyterNotebookApp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned how the key use of KubeDirector applications, the KubeDirector clusters, and the KubeDirector Connections capability is what makes your ML pipeline very **dynamic**.\n",
    "\n",
    "* [Conclusion](7-Conclusion.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
