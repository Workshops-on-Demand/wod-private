{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Persistent Storage Basics\n",
    "\n",
    "This workshop assumes a basic knowledge of Kubernetes and some familiarity with persistent storage. You should've received a briefing about the core concepts and an introductory presentation to the workshop nomenclatures we're using prior to accessing the labs.\n",
    "\n",
    "## Get started!\n",
    "\n",
    "In this workshop, we'll utilize the \"hpe-standard\" `StorageClass`. It uses the HPE CSI Driver. It's common in Kubernetes application deployment to not have more than one `StorageClass` and for that `StorageClass` to be marked \"default\". That way users don't have to call out which `StorageClass` to use. Since the HPE Data Fabric is marked \"default\" on the HPE Ezmeral Container Platform, we need to be explicit. \n",
    "\n",
    "Let's inspect the `StorageClasses` available on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get storageclasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the \"hpe-standard\" `StorageClass` explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get storageclasses/hpe-standard -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters prefixed with \"csi.storage.k8s.io\" are needed for the CSI sidecars to communicate with the HPE CSI Driver, both for the Controller and Node driver. The remaining parameters are specific to the HPE CSI Driver *or* the backend CSP. If you want to study which parameters goes to the HPE CSI Driver and what parameters are available for each of the supported backends, check out these resources on the HPE Storage Container Orchestrator Documentation (SCOD) portal.\n",
    "\n",
    "- [Base `StorageClass` parameters for the HPE CSI Driver](https://scod.hpedev.io/csi_driver/using.html#base_storageclass_parameters)\n",
    "- [HPE Nimble Storage parameters](https://scod.hpedev.io/container_storage_provider/hpe_nimble_storage/index.html#storageclass_parameters)\n",
    "- [HPE Primera and HPE 3PAR parameters](https://scod.hpedev.io/container_storage_provider/hpe_3par_primera/index.html#storageclass_parameters)\n",
    "\n",
    "Other `StorageClass` attributes that are of interest from an end-user perspective is the `.reclaimPolicy` (what happens to the `PersistentVolume` when the `PersistentVolumeClaim` is deleted) and `.allowVolumeExpansion` (if the `StorageClass` allows volume expansion or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your first claim\n",
    "\n",
    "Review the `PersistentVolumeClaim` below.\n",
    "\n",
    "---\n",
    "```yaml\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: my-first-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 128Mi\n",
    "  storageClassName: hpe-standard\n",
    "```\n",
    "---\n",
    "\n",
    "In the `.spec`, the `.spec.accessModes` and `.spec.resources.requests.storage` are mandatory. `.spec.storageClassName` is necessary in our case as we specifically want to use the \"hpe-standard\" `StorageClass`. The different access modes are discussed later in the workshop. We'll use `ReadWriteOnce` until then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f obj/my-first-pvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our newly created PVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get persistentvolumeclaim/my-first-pvc -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important aspects of the claim is the `.spec.status.phase` where it should say \"Bound\". It may be say \"Pending\" if the cluster or backend is busy. Rerun the previous command to ensure the phase says \"Bound\" before proceeding.\n",
    "\n",
    "It's also possible to observe the backing `PersistentVolume` in `.spec.volumeName`. However, since `PersistentVolumes` are cluster-wide objects, restricted users are only allowed to observe the name by reference through the `PersistentVolumeClaim`.\n",
    "\n",
    "Also, pay attention to `.spec.volumeMode`, which by default is set to \"Filesystem\". Later in the lab, we'll create a `PersistentVolumeClaim` where we request \"Block\" and show you how to access the raw block device from a `Pod`.\n",
    "\n",
    "## Attach a PersistentVolumeClaim to a workload\n",
    "\n",
    "We should now have a `PersistentVolumeClaim` named \"my-first-pvc\". To attach it to a workload, it needs to be referenced in the `.spec.volumes` stanza of the controller. Let's create a `Deployment` referencing our claim.\n",
    "\n",
    "---\n",
    "```yaml\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: my-first-deploy\n",
    "  labels:\n",
    "    app: my-first-deploy\n",
    "spec:\n",
    "  replicas: 1\n",
    "  strategy:\n",
    "    type: Recreate\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: my-ioping\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: my-ioping\n",
    "    spec:\n",
    "      containers:\n",
    "      - image: hpestorage/ioping\n",
    "        name: ioping\n",
    "        command: [ \"ioping\" ]\n",
    "        args: [ \"/data\" ]\n",
    "        volumeMounts:\n",
    "          - name: my-data\n",
    "            mountPath: /data\n",
    "      volumes:\n",
    "      - name: my-data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: my-first-pvc\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f obj/my-first-deploy.yaml\n",
    "kubectl rollout status deploy/my-first-deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should take a few seconds or so before the `Deployment` comes up. Run the below command when it says `1/1` in the \"READY\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get deployments/my-first-deploy -o wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Pod` we declared in the `Deployment` sends a 4KiB IO to the specified mountpoint and sends the results to `stdout`. To verify the `Pod` is indeed working, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl logs --since=5s deployments/my-first-deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to observe a 4KiB IO sent to the `/data` mountpoint that hosts a XFS filesystem on a multipath device.\n",
    "\n",
    "Leave the `Deployment` running as we go through the next exercise.\n",
    "\n",
    "## Resize a PersistentVolumeClaim\n",
    "\n",
    "Resizing a `PersistentVolumeClaim` is done with either the `kubectl edit` or `kubectl patch` sub-command. Since we're using Jupyter notebooks, \"patch\" is more practical. Let's inspect the current `PersistentVolumeClaim` size from the workload's perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it deployment/my-first-deploy -- df -h /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double the volume size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl patch persistentvolumeclaim my-first-pvc -p '{\"spec\":{\"resources\":{\"requests\":{\"storage\": \"256Mi\"}}}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, wait a couple of minutes (skip to the next exercise and come back). Then re-run the `df` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl exec -it deployment/my-first-deploy -- df -h /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new size should be reflected on the device. How does it compare to the previous output?\n",
    "\n",
    "# Working with Raw Block Devices\n",
    "\n",
    "CSI is capable of exposing a representation of the underlying block device to the `Pod`. By default, the `.spec.volumeMode` of a `PersistentVolumeClaim` is set to \"Filesystem\". Let's create a block-based `PersistentVolumeClaim`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```yaml\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: my-block-pvc\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 128Mi\n",
    "  storageClassName: hpe-standard\n",
    "  volumeMode: Block\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f obj/my-block-pvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Deployment` that accesses the block-based `PersistentVolumeClaim`.\n",
    "\n",
    "---\n",
    "```yaml\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: my-block-deploy\n",
    "  labels:\n",
    "    app: my-block-deploy\n",
    "spec:\n",
    "  replicas: 1\n",
    "  strategy:\n",
    "    type: Recreate\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: my-block-ioping\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: my-block-ioping\n",
    "    spec:\n",
    "      containers:\n",
    "      - image: hpestorage/ioping\n",
    "        name: ioping\n",
    "        command: [ \"ioping\" ]\n",
    "        args: [ \"/dev/xvda\" ]\n",
    "        volumeDevices:\n",
    "          - name: my-data\n",
    "            devicePath: /dev/xvda\n",
    "      volumes:\n",
    "      - name: my-data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: my-block-pvc\n",
    "```\n",
    "---\n",
    "\n",
    "Pay attention to a few key differences versus using `.spec.volumeMode: Filesystem` in the `PersistentVolumeClaim`. In the `Deployment` stanza, `.spec.template.spec.containers`, you'll see `volumeDevices` and `devicePath` instead of `volumeMounts` and `mountPath`.\n",
    "\n",
    "Go ahead and create the deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f obj/my-block-deploy.yaml\n",
    "kubectl rollout status deploy/my-block-deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `Pod` comes up, check the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl logs --since=5s deployments/my-block-deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IO is now being issues against the block device exposed to the `Pod` instead of the filesystem.\n",
    "\n",
    "**Question:** Is the IO faster, slower or similar to the one issued in the case of `volumeMode: Filesystem`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up!\n",
    "\n",
    "We'll clean up after you eventually, but if you would be so kind as to free up some resources before we skip to the next set of exercises, it would be greatly appreciated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete deploy/my-first-deploy deploy/my-block-deploy pvc/my-first-pvc pvc/my-block-pvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's continue on to [Lab 2: Snapshots and Clones](2-WKSHP-CSI-DataManagement.ipynb)\n",
    "\n",
    "> **Learn more**\n",
    "> - {{ BRANDING }} Community Blog: [Introducing a multi-vendor CSI driver for Kubernetes](https://developer.hpe.com/blog/n0J8kpk1DJf4y7xD2D4X/introducing-a-multi-vendor-csi-driver-for-kubernetes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
