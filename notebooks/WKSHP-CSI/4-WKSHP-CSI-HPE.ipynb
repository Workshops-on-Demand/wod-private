{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Non-portable HPE CSI Driver Capabilities\n",
    "\n",
    "The HPE CSI Driver contains a number of practical features that aren't portable within CSI drivers and some are capabilities that are even unique for each of the backend CSPs that the HPE CSI Driver supports.\n",
    "\n",
    "## The StorageClass\n",
    "\n",
    "Let's inspect our `StorageClass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get storageclass/hpe-standard -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.parameters.allowOverrides` is a comma-separated list of parameters that users may override by annotating the `PersistentVolumeClaim` at creation.\n",
    "\n",
    "---\n",
    "```yaml\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: my-override-pvc\n",
    "  annotations:\n",
    "    csi.hpe.com/limitMbps: \"5\"\n",
    "    csi.hpe.com/description: \"My Batch Job\"\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 32Mi\n",
    "  storageClassName: hpe-standard\n",
    "  volumeMode: Block\n",
    "```\n",
    "---\n",
    "\n",
    "The idea here is to create a throttled volume to run a batch `Job`. We don't want to impact any workloads, hence throttling the backed volume to 5MB/s.\n",
    "\n",
    "The `Job` looks like this.\n",
    "\n",
    "---\n",
    "```yaml\n",
    "---\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: my-override-job\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: dd\n",
    "        image: hpestorage/ioping\n",
    "        command: [ \"dd\" ]\n",
    "        args: [ \"if=/dev/xvda\", \"of=/dev/null\", \"iflag=direct\", \"bs=32k\" ]\n",
    "        volumeDevices:\n",
    "          - name: my-data\n",
    "            devicePath: /dev/xvda\n",
    "      volumes:\n",
    "      - name: my-data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: my-override-pvc\n",
    "      restartPolicy: Never\n",
    "  backoffLimit: 4\n",
    "```\n",
    "---\n",
    "\n",
    "Let's create the `PersistentVolumeClaim` and `Job`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f obj/my-override-pvc.yaml -f obj/my-override-job.yaml\n",
    "kubectl wait --for=condition=complete --timeout=600s job/my-override-job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a minute or so for the `Job` to complete. Check the logs once it finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl logs job.batch/my-override-job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughput should be around the domain of 5MB/s. It could be less, if the environment is busy.\n",
    "\n",
    "Now, inspecting the `StorageClass` above further, we can observe `.parameters.allowMutations`. This allows users to modify an attribute of a `PersistentVolumeClaim` once it has been provisioned. Let's change our 5MB/s to 10MB/s and observe the results on the next submitted `Job`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl annotate --overwrite pvc/my-override-pvc csi.hpe.com/limitMbps=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a new `Job`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl replace --force -f obj/my-override-job.yaml\n",
    "kubectl wait --for=condition=complete --timeout=600s job/my-override-job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's inspect the `Job` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl logs job.batch/my-override-job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was the `Job` completed faster than previously?\n",
    "\n",
    "Let's delete our `Job` and `PersistentVolumeClaim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete job/my-override-job pvc/my-override-pvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Modes\n",
    "\n",
    "By default, the CSP backends supported by the HPE CSI Drivers are block storage based. That means only one `Pod` at any given time can access the volume and `PersistentVolumeClaims` need to be created with `.spec.accessMode.ReadWriteOnce`. However, the HPE CSI Driver includes a NFS Server Provisioner that allows both `ReadWriteMany` and `ReadOnlyMany` in the `PersistentVolumeClaim`. The NFS Server Provisioner will run a NFS server on top of a `ReadWriteOnce` claim on the Kubernetes cluster itself to allow access from multiple `Pods` across multiple nodes using the standard NFSv4 protocol.\n",
    "\n",
    "The `StorageClass` parameter `nfsResources` controls whether to create a NFS `Deployment` and associated resources to serve the claim.\n",
    "\n",
    "**Note:** All claims, including `ReadWriteOnce`, will provision resource, hence we want to control it granularly per claim.\n",
    "\n",
    "Assume the following claim, using the \"hpe-standard\" `StorageClass`.\n",
    "\n",
    "---\n",
    "```yaml\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: my-nfs-pvc\n",
    "  annotations:\n",
    "    csi.hpe.com/nfsResources: \"true\"\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 512Mi\n",
    "  storageClassName: hpe-standard\n",
    "```\n",
    "---\n",
    "\n",
    "It takes a while to create claim, as additional resources needs to be created (all happening transparently for the user).\n",
    "\n",
    "Let's create the claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f obj/my-nfs-pvc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring up a five replica `Deployment`.\n",
    "\n",
    "---\n",
    "```yaml\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: my-nfs-deploy\n",
    "  labels:\n",
    "    app: my-nfs-deploy\n",
    "spec:\n",
    "  replicas: 5\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: my-ioping\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: my-ioping\n",
    "    spec:\n",
    "      containers:\n",
    "      - image: hpestorage/ioping\n",
    "        name: ioping\n",
    "        command: [ \"ioping\" ]\n",
    "        args: [ \"/data\" ]\n",
    "        volumeMounts:\n",
    "          - name: my-data\n",
    "            mountPath: /data\n",
    "      volumes:\n",
    "      - name: my-data\n",
    "        persistentVolumeClaim:\n",
    "          claimName: my-nfs-pvc\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl create -f obj/my-nfs-deploy.yaml\n",
    "kubectl rollout status deploy/my-nfs-deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for all five `Pods` to come up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods -l app=my-multiping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, inspect the logs of each `Pod` to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods -o name | xargs -n 1 kubectl logs --since=1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it's evident that the `Pods` are accessing a NFS mountpoint hosted by a NFS server in the Kubernetes cluster. This might be obvious, but, then again, it might not be depending on the load of the cluster. Why is one `Pod` faster than the other four?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tidy up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete deploy/my-nfs-deploy pvc/my-nfs-pvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "First, congratulations! You've concluded the [{{ BRANDINGWOD }} CSI Workshop-on-Demand](https://developer.hpe.com/hackshack/workshop/2). Second, the HPE CSI Driver has huge potential to improve business outcomes for data centric workloads if used correctly. Learn more about the different backends that are supported and how to get started by deploying your own Kubernetes cluster and installing the driver. Everything you need is available on the [SCOD](https://scod.hpedev.io/csi_driver/using.html#using_the_nfs_server_provisioner) portal.\n",
    "\n",
    "Now, let's [conclude this workshop](5-WKSHP-CSI-Conclusion.ipynb)!\n",
    "\n",
    "> **Learn more** \n",
    "> - NFS Server Provisioner on [HPE Storage Container Orchestrator Documentation](https://scod.hpedev.io/csi_driver/using.html#using_the_nfs_server_provisioner) (SCOD).\n",
    "> - {{ BRANDING }} Community Blog: [Introducing an NFS Server Provisioner for the HPE CSI Driver for Kubernetes](https://developer.hpe.com/blog/xABwJY56qEfNGMEo1lDj/introducing-an-nfs-server-provisioner-for-the-hpe-csi-driver-for-kuberne)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
