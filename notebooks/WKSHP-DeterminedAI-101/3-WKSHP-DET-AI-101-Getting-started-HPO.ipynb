{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5e4bcf-e5b6-40a3-9135-b8d583505348",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Getting Started with Determined, the Open-Source Deep Learning Training Platform - Lab 3\n",
    "## Hyperparameter optimization with Determined without requiring any model code changes\n",
    "\n",
    "In the previous part of the hands-on lab, you learned how to easily distribute a training task across multiple GPUs. In this section, you will look at another way that an experiment can profit from multiple GPUs: the ***automatic model tuning***, also known as hyperparameter tuning or hyperparameter optimization (**HPO**). HPO finds the best version of a model by running many training tasks (trials) on your dataset using the Searcher algorithm and ranges of hyperparameters that you specify in the experiment configuration file. Determined then chooses the hyperparameter values that result in a model that performs the best, as measured by a validation metric that you define in the experiment configuration file.\n",
    "\n",
    "So, in this part of the lab, let's run an experiment with the same model code, but this time leverage Determined's hyperparameter optimization that ML engineers typically use to improve the model accuracy and efficiently find the combination of hyperparameter values that yields the best-performing model.  Here in this lab, the hyperparameters in the experiment configuration file are specified as ranges rather than fixed values, and the `Adaptive ASHA` searcher method is used to explore the hyperparameter space, helping you find the best hyperparameters for your model.\n",
    "\n",
    "**With HPO, an experiment consists of multiple training tasks (trials)** running simultaneously on different GPUs. Each of the trials trains the model on the same dataset and code for the DL model. However, each trial uses a different configuration of hyperparameters **randomly** chosen by the Searcher from the range of values that you specified in the experiment configuration file.\n",
    "\n",
    "For this part of lab, the number of trials to run, the set of user-defined hyperparameters range, the searcher method, and the amount of data (batches or epochs) on which to train the model are defined in the experiment configuration file _adaptive.yaml_.\n",
    "\n",
    ">Note: The _Adaptive ASHA_ searcher is a state-of-the-art method that is used to find effective hyperparameter settings within a predifined range of hyperparameter values.\n",
    "\n",
    "***More about Hyperparameter optimization and Searcher methods supported by Determined can be found [here](https://docs.determined.ai/latest/training-hyperparameter/index.html#hyperparameter-tuning)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab41b4a-d8b2-4b09-a064-cec710e91ef6",
   "metadata": {},
   "source": [
    "### 1- Create an experiment to train multiple models as part of a hyperparameter search, using Determined hyperparameter optimization (HPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696291e-bcba-48a1-9ca6-ec1f88a9846c",
   "metadata": {},
   "source": [
    "Let's run an experiment with the same model definition (same code), but this time leveraging Determined's hyperparameter optimization functionality using the _adaptive.yaml_ experiment configuration file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92247a4-4e2e-416f-ba7d-4fe5f568b9c2",
   "metadata": {},
   "source": [
    "#### First, let's take a closer look at the experiment configuration file for HPO: adaptive.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f45091-4fb7-44a1-8529-18280f25cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat Code/adaptive.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b4423-1fe0-4c20-9925-8007455e8033",
   "metadata": {},
   "source": [
    "As you can see here, you set up the experiment configuration file a bit differently than previous experiments. The experiment configuration file _adaptive.yaml_ tells Determined to use ***adaptive_asha*** as Searcher algorithm and the range of values to explore for each hyperparameter. In the searcher section, the parameter ***max_trials*** indicates the total number of trials that the experiment will create and how many model configuration to explore. Each trial runs on one GPU because the resource parameter _slot_per_trial_ is not specified, therefore the default setting of _slot_per_trial=1_ is used. \n",
    "\n",
    ">**Note:** Adaptive_ASHA method works best with many hundreds of trials. For the purpose of this hands-on lab, the maximum number of trials is set to 6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc85db-ffbf-4e5b-8dd8-a727c53c2abd",
   "metadata": {},
   "source": [
    "#### Next, submit the experiment with the experiment configuration file _adaptive.yaml_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf46ae-1152-4ace-8839-dc3c6d55088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first define the DET_MASTER env variable:\n",
    "masterUrl=$(kubectl describe service determined-master-service-stagingdetai -n determinedai | grep gateway/8080 | awk '{print $3}')\n",
    "determined_master=\"http://${masterUrl}\"\n",
    "export DET_MASTER=${determined_master}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e8b41-60aa-44c3-a69f-445d413849b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch experiment to train the model with hyperparameter tuning (HPO)\n",
    "det experiment create Code/adaptive.yaml Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5840e24-bc40-4f57-8ba9-0c978b1b78bb",
   "metadata": {},
   "source": [
    "In the lab environment, each Kubernetes worker host has one GPU only. Therefore, each training task (trial) in the experiment will run on one GPU. \n",
    "\n",
    ">**Note:** In an environment with many multi-GPU devices, you could combine HPO and Distributing Training and assign more than one GPU to each trial in the experiment by defining the parameter _slot_per_trial_ in the experiment configuration file much like the Distributed Training you examined earlier.\n",
    "\n",
    "#### Using the command below, you will see that Determined Master has scheduled multiple trials for your experiment in the Kubernetes cluster, each of which will use its own GPU. The POD name (one per trial) for your experiment is in the form:\n",
    "\n",
    " _exp-\\<experimentID\\>-trial-\\<TriaID\\>-\\<unique-name\\>_\n",
    "\n",
    "> Notice the trial PODs have been assigned a different trial ID for your experiment, which means your experiment features multiple trials each with a different set of hyperparameters. \n",
    "\n",
    "> <font color=\"blue\"> **Note:** As you are sharing the same Kubernetes resources with other participants, and depending on the number of concurrent experiments running, your training tasks PODs might be in **Pending** state waiting for GPU resources to become available in the Kubernetes cluster. You might need to wait a few minutes until other experiments complete for your training tasks PODs to become **Running**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610cf4e2-b408-4e2d-93b0-6658fc9a0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get pods -n determinedai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d543ff43-cd23-4f57-af3b-76fc8af6cd28",
   "metadata": {},
   "source": [
    "#### Run the code cell below to monitor the execution progress of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39cfafa-83db-4658-8f5b-3c448c6f8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "det experiment list | tail -1\n",
    "# Get the experiment Id, remove spaces\n",
    "myexpId=$(det experiment list | tail -1 | cut -d'|' -f 1 |  tr -d ' ')\n",
    "#det experiment describe ${myexpId} --json | jq .[0].state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d0800-e1cb-4eba-9ec9-6157b559edc6",
   "metadata": {},
   "source": [
    "### 2- Monitor and visualize your experiment using the Determined Web User Interface\n",
    "\n",
    "Determined will run the number of _max_trials_ trials and automatically start new trials as resources become available.\n",
    "\n",
    "To monitor the progress of the training task and access information on both training and validation performance for the trials of your experiment, you can simply return to the Determined **WebUI**.\n",
    "\n",
    "##### From the **Dashboard**, after a minute or so, you should see the experiment as an **active** state and the completion percentage. \n",
    "\n",
    "> <font color=\"blue\"> **Important Note:** If there are multiple concurrent participants to the workshop, your experiment might not run yet because there are more experiments running than the Kubernetes cluster has GPUs. You might need to wait a few minutes until other experiments complete for your experiment to start running. </font>\n",
    "\n",
    "##### Select your most recent experiment.\n",
    "\n",
    "As you can see in the **Visualization** pane, Determinedâ€™s hyperparameter optimization provides you with several visualization options for analyzing results: Learning Curve, Parallel Plot, Scatter Plot, Heatmap.\n",
    "\n",
    ">***Note:*** To learn more about these visualization options, check out the blog post [here](https://www.determined.ai/blog/hyperparameter-visualizations-determined).\n",
    "\n",
    "<img src=\"Pictures/WebUI-Exp-adaptive-visualization.png\" height=\"154\" width=\"900\">\n",
    "\n",
    ">Note: You can navigate to **Trials** tab to see progress status of the training tasks for your experiment.\n",
    "\n",
    "As the experiment runs, the _Learning Curve_ graph is showing the model validation accuracy metric (_val_categorical_accuracy_). From the **Metric** drop-down list, under **Training Metrics**, select _categorical_accuracy_. Click the ***Apply*** button as shown in the picture above to visualize the model accuracy on training data for each trial over the number of completed batches. \n",
    "\n",
    "<img src=\"Pictures/WebUI-Exp-adaptive-graphs.png\" height=\"394\" width=\"900\">\n",
    "\n",
    "After the experiment is complete, you might see that Determined's hyperparameter Searcher Adaptive ASHA's ***early stopping*** capability has stopped poor performing trials that do not require any extra training. Determined releases valuable GPU resources on trials that will never produce the best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a7c388-54a6-4f8f-b3a1-73b605948c7e",
   "metadata": {},
   "source": [
    "### 3 - Get the trial and hyperparameters that yields to the best model\n",
    "\n",
    "Like the other experiments you explored earlier, you can use the command below to list the trial that yields to the best model:\n",
    "\n",
    "* _det experiment list-checkpoints [--best] [N best checkpoints to return] \\<experiment_Id\\>_\n",
    "\n",
    "#### Run the code cell below to display the trial that yields to the best model for your experiment\n",
    "\n",
    "You can see on the experiment detail page that training the model with the hyperparameter settings in `adaptive.yaml` yields a validation accuracy between 93% and 97%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59096343-04b4-47a5-aab4-30244e8b1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the best Trial checkpoint(s) (training task):\n",
    "det experiment list-checkpoints --best 1 ${myexpId}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd051f-871c-4704-a2be-3ea72aac1d80",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can use the command below to discover the hyperparameters that yield to the best model:\n",
    "\n",
    "* _det trial describe \\<trial_Id\\>_\n",
    "\n",
    "#### Run the code cell below to display the hyperparameters that yield the best model for your experiment\n",
    ">**Note**: You might see an SQL error message. You can ignore the issue and continue with the next step to reclaim some storage space. Then, if time permits, you may want to launch a new HPO experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4461a5-c127-4cd7-b358-c49f00e27f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestTrialId=$(det experiment list-checkpoints --best 1 ${myexpId} | head -3 | tail -1 | cut -d'|' -f 1 |  tr -d ' ')\n",
    "echo \"Best Trial ID: \" $bestTrialId\n",
    "det trial describe ${bestTrialId}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52f6c59-7da5-4fd3-8b9e-fdd15e3926f7",
   "metadata": {},
   "source": [
    ">**Note:** Unlike the non-HPO experiments you explored earlier, the _adaptive.yaml_ experiment configuration file does not define a periodic validation parameter (min_validation_period). The validated model is checkpointed at the trial end. Adaptive_ASHA method could also automatically checkpoint a model earlier if it makes sense to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f5d7d0-e7ed-4706-92c0-d9bb020a4f45",
   "metadata": {},
   "source": [
    "### 4- Delete the checkpoints to reclaim storage space in the storage file system\n",
    "\n",
    "The default **checkpoint garbage collection policy** dictates Determined to save the most recent and the best checkpoint per training task (trial). The ***save_experiment_best***, ***save_trial_best*** and ***save_trial_latest*** parameters specify which checkpoints to save. The default policy is set as follows:\n",
    "\n",
    "  * save_experiment_best:0 \n",
    "  * save_trial_best:1\n",
    "  * save_trial_latest:1\n",
    " \n",
    "#### Run the code cell below to reclaim some storage disk space by changing the default checkpoint garbage collection policy as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39e584-2caf-4fe1-bb8b-a72f1b88888a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the checkpoints data for the HPO training\n",
    "det experiment set gc-policy --yes --save-experiment-best 0 --save-trial-best 0 --save-trial-latest 0 ${myexpId}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f8a59-040e-489c-b114-96820feaf017",
   "metadata": {},
   "source": [
    "#### Wrap up of the workshop.\n",
    "Click on **Conclusion** below to open the Conclusion notebook. \n",
    "* [Conclusion](4-WKSHP-Conclusion.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
